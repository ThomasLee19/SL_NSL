{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "train_dataset_path = '/root/autodl-tmp/projects/SL_NSL/dataset/transformed/KDDTrain+.csv'\n",
    "test_dataset_path = '/root/autodl-tmp/projects/SL_NSL/dataset/transformed/KDDTest+.csv'\n",
    "output_directory = '/root/autodl-tmp/projects/SL_NSL/dataset/processed/multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mappings for multiclass classification:\n",
      "Class 0: Normal Traffic\n",
      "Class 1: DOS (Denial of Service)\n",
      "Class 2: Probe (Surveillance/Scanning)\n",
      "Class 3: R2L (Remote to Local)\n",
      "Class 4: U2R (User to Root)\n",
      "\n",
      "Detailed attack mappings:\n",
      "back: Class 1 (DOS (Denial of Service))\n",
      "buffer_overflow: Class 4 (U2R (User to Root))\n",
      "ftp_write: Class 3 (R2L (Remote to Local))\n",
      "guess_passwd: Class 3 (R2L (Remote to Local))\n",
      "imap: Class 3 (R2L (Remote to Local))\n",
      "ipsweep: Class 2 (Probe (Surveillance/Scanning))\n",
      "land: Class 1 (DOS (Denial of Service))\n",
      "loadmodule: Class 4 (U2R (User to Root))\n",
      "mscan: Class 2 (Probe (Surveillance/Scanning))\n",
      "multihop: Class 3 (R2L (Remote to Local))\n",
      "neptune: Class 1 (DOS (Denial of Service))\n",
      "nmap: Class 2 (Probe (Surveillance/Scanning))\n",
      "normal: Class 0 (Normal Traffic)\n",
      "perl: Class 4 (U2R (User to Root))\n",
      "phf: Class 3 (R2L (Remote to Local))\n",
      "pod: Class 1 (DOS (Denial of Service))\n",
      "portsweep: Class 2 (Probe (Surveillance/Scanning))\n",
      "rootkit: Class 4 (U2R (User to Root))\n",
      "saint: Class 2 (Probe (Surveillance/Scanning))\n",
      "satan: Class 2 (Probe (Surveillance/Scanning))\n",
      "smurf: Class 1 (DOS (Denial of Service))\n",
      "spy: Class 3 (R2L (Remote to Local))\n",
      "sqlattack: Class 4 (U2R (User to Root))\n",
      "teardrop: Class 1 (DOS (Denial of Service))\n",
      "warezclient: Class 3 (R2L (Remote to Local))\n",
      "warezmaster: Class 3 (R2L (Remote to Local))\n",
      "xterm: Class 4 (U2R (User to Root))\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define attck type\n",
    "# Define attack type mappings\n",
    "attack_mapping = {\n",
    "    # Normal (0)\n",
    "    'normal': 0,\n",
    "    \n",
    "    # DOS attacks (1)\n",
    "    'back': 1,\n",
    "    'land': 1,\n",
    "    'neptune': 1,\n",
    "    'pod': 1,\n",
    "    'smurf': 1,\n",
    "    'teardrop': 1,\n",
    "    \n",
    "    # Probe attacks (2)\n",
    "    'ipsweep': 2,\n",
    "    'nmap': 2,\n",
    "    'portsweep': 2,\n",
    "    'satan': 2,\n",
    "    'mscan': 2,\n",
    "    'saint': 2,\n",
    "    \n",
    "    # R2L attacks (3)\n",
    "    'ftp_write': 3,\n",
    "    'guess_passwd': 3,\n",
    "    'imap': 3,\n",
    "    'multihop': 3,\n",
    "    'phf': 3,\n",
    "    'spy': 3,\n",
    "    'warezclient': 3,\n",
    "    'warezmaster': 3,\n",
    "    \n",
    "    # U2R attacks (4)\n",
    "    'buffer_overflow': 4,\n",
    "    'loadmodule': 4,\n",
    "    'perl': 4,\n",
    "    'rootkit': 4,\n",
    "    'sqlattack': 4,\n",
    "    'xterm': 4\n",
    "}\n",
    "\n",
    "# Define class names for printing\n",
    "class_names = {\n",
    "    0: \"Normal Traffic\",\n",
    "    1: \"DOS (Denial of Service)\",\n",
    "    2: \"Probe (Surveillance/Scanning)\",\n",
    "    3: \"R2L (Remote to Local)\",\n",
    "    4: \"U2R (User to Root)\"\n",
    "}\n",
    "\n",
    "# Print class mappings\n",
    "print(\"Class mappings for multiclass classification:\")\n",
    "for class_id, class_name in class_names.items():\n",
    "    print(f\"Class {class_id}: {class_name}\")\n",
    "print(\"\\nDetailed attack mappings:\")\n",
    "for attack, class_id in sorted(attack_mapping.items()):\n",
    "    print(f\"{attack}: Class {class_id} ({class_names[class_id]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training dataset...\n",
      "Training dataset shape: (125973, 42)\n",
      "\n",
      "Sample of training data:\n",
      "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
      "0         0           tcp  ftp_data   SF        491          0     0   \n",
      "1         0           udp     other   SF        146          0     0   \n",
      "2         0           tcp   private   S0          0          0     0   \n",
      "3         0           tcp      http   SF        232       8153     0   \n",
      "4         0           tcp      http   SF        199        420     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  25   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  26   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                 255   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.17                    0.03   \n",
      "1                    0.00                    0.60   \n",
      "2                    0.10                    0.05   \n",
      "3                    1.00                    0.00   \n",
      "4                    1.00                    0.00   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.17                         0.00   \n",
      "1                         0.88                         0.00   \n",
      "2                         0.00                         0.00   \n",
      "3                         0.03                         0.04   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.05   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  1.00                      1.00                  0.00   \n",
      "3                  0.03                      0.01                  0.00   \n",
      "4                  0.00                      0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_rerror_rate    label  \n",
      "0                      0.00   normal  \n",
      "1                      0.00   normal  \n",
      "2                      0.00  neptune  \n",
      "3                      0.01   normal  \n",
      "4                      0.00   normal  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "Data types:\n",
      "duration                         int64\n",
      "protocol_type                   object\n",
      "service                         object\n",
      "flag                            object\n",
      "src_bytes                        int64\n",
      "dst_bytes                        int64\n",
      "land                             int64\n",
      "wrong_fragment                   int64\n",
      "urgent                           int64\n",
      "hot                              int64\n",
      "num_failed_logins                int64\n",
      "logged_in                        int64\n",
      "num_compromised                  int64\n",
      "root_shell                       int64\n",
      "su_attempted                     int64\n",
      "num_root                         int64\n",
      "num_file_creations               int64\n",
      "num_shells                       int64\n",
      "num_access_files                 int64\n",
      "num_outbound_cmds                int64\n",
      "is_host_login                    int64\n",
      "is_guest_login                   int64\n",
      "count                            int64\n",
      "srv_count                        int64\n",
      "serror_rate                    float64\n",
      "srv_serror_rate                float64\n",
      "rerror_rate                    float64\n",
      "srv_rerror_rate                float64\n",
      "same_srv_rate                  float64\n",
      "diff_srv_rate                  float64\n",
      "srv_diff_host_rate             float64\n",
      "dst_host_count                   int64\n",
      "dst_host_srv_count               int64\n",
      "dst_host_same_srv_rate         float64\n",
      "dst_host_diff_srv_rate         float64\n",
      "dst_host_same_src_port_rate    float64\n",
      "dst_host_srv_diff_host_rate    float64\n",
      "dst_host_serror_rate           float64\n",
      "dst_host_srv_serror_rate       float64\n",
      "dst_host_rerror_rate           float64\n",
      "dst_host_srv_rerror_rate       float64\n",
      "label                           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and display initial data\n",
    "print(\"\\nLoading training dataset...\")\n",
    "df_train = pd.read_csv(train_dataset_path)\n",
    "\n",
    "# Display initial information\n",
    "print(\"Training dataset shape:\", df_train.shape)\n",
    "print(\"\\nSample of training data:\")\n",
    "print(df_train.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of scaled training data:\n",
      "   duration protocol_type   service flag  src_bytes  dst_bytes      land  \\\n",
      "0 -0.110249           tcp  ftp_data   SF  -0.007679  -0.004919 -0.014089   \n",
      "1 -0.110249           udp     other   SF  -0.007737  -0.004919 -0.014089   \n",
      "2 -0.110249           tcp   private   S0  -0.007762  -0.004919 -0.014089   \n",
      "3 -0.110249           tcp      http   SF  -0.007723  -0.002891 -0.014089   \n",
      "4 -0.110249           tcp      http   SF  -0.007728  -0.004814 -0.014089   \n",
      "\n",
      "   wrong_fragment    urgent       hot  ...  dst_host_srv_count  \\\n",
      "0       -0.089486 -0.007736 -0.095076  ...           -0.818890   \n",
      "1       -0.089486 -0.007736 -0.095076  ...           -1.035688   \n",
      "2       -0.089486 -0.007736 -0.095076  ...           -0.809857   \n",
      "3       -0.089486 -0.007736 -0.095076  ...            1.258754   \n",
      "4       -0.089486 -0.007736 -0.095076  ...            1.258754   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0               -0.782367               -0.280282   \n",
      "1               -1.161030                2.736852   \n",
      "2               -0.938287               -0.174417   \n",
      "3                1.066401               -0.439078   \n",
      "4                1.066401               -0.439078   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                     0.069972                    -0.289103   \n",
      "1                     2.367737                    -0.289103   \n",
      "2                    -0.480197                    -0.289103   \n",
      "3                    -0.383108                     0.066252   \n",
      "4                    -0.480197                    -0.289103   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0             -0.639532                 -0.624871             -0.224532   \n",
      "1             -0.639532                 -0.624871             -0.387635   \n",
      "2              1.608759                  1.618955             -0.387635   \n",
      "3             -0.572083                 -0.602433             -0.387635   \n",
      "4             -0.639532                 -0.624871             -0.387635   \n",
      "\n",
      "   dst_host_srv_rerror_rate    label  \n",
      "0                 -0.376387   normal  \n",
      "1                 -0.376387   normal  \n",
      "2                 -0.376387  neptune  \n",
      "3                 -0.345084   normal  \n",
      "4                 -0.376387   normal  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Feature Scaling\n",
    "def feature_scaling(df, scaler=None):\n",
    "    \"\"\"\n",
    "    Scale numerical features using StandardScaler\n",
    "    Returns scaled dataframe and scaler object\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Fit and transform\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "# Apply scaling to training data\n",
    "df_train, scaler = feature_scaling(df_train)\n",
    "print(\"\\nSample of scaled training data:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of data after dummy transformation:\n",
      "   duration  src_bytes  dst_bytes      land  wrong_fragment    urgent  \\\n",
      "0 -0.110249  -0.007679  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "1 -0.110249  -0.007737  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "2 -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "3 -0.110249  -0.007723  -0.002891 -0.014089       -0.089486 -0.007736   \n",
      "4 -0.110249  -0.007728  -0.004814 -0.014089       -0.089486 -0.007736   \n",
      "\n",
      "        hot  num_failed_logins  logged_in  num_compromised  ...  flag_REJ  \\\n",
      "0 -0.095076          -0.027023  -0.809262        -0.011664  ...         0   \n",
      "1 -0.095076          -0.027023  -0.809262        -0.011664  ...         0   \n",
      "2 -0.095076          -0.027023  -0.809262        -0.011664  ...         0   \n",
      "3 -0.095076          -0.027023   1.235694        -0.011664  ...         0   \n",
      "4 -0.095076          -0.027023   1.235694        -0.011664  ...         0   \n",
      "\n",
      "   flag_RSTO  flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  \\\n",
      "0          0            0          0        0        0        0        0   \n",
      "1          0            0          0        0        0        0        0   \n",
      "2          0            0          0        1        0        0        0   \n",
      "3          0            0          0        0        0        0        0   \n",
      "4          0            0          0        0        0        0        0   \n",
      "\n",
      "   flag_SF  flag_SH  \n",
      "0        1        0  \n",
      "1        1        0  \n",
      "2        0        0  \n",
      "3        1        0  \n",
      "4        1        0  \n",
      "\n",
      "[5 rows x 123 columns]\n",
      "New shape after creating dummy variables: (125973, 123)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Get Dummies Transformation\n",
    "def get_dummies_transform(df):\n",
    "    \"\"\"\n",
    "    Convert categorical variables to dummy/indicator variables\n",
    "    \"\"\"\n",
    "    categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "    df_dummy = pd.get_dummies(df, columns=categorical_columns, dtype=int)\n",
    "    return df_dummy\n",
    "\n",
    "# Apply get_dummies transformation\n",
    "df_train = get_dummies_transform(df_train)\n",
    "print(\"\\nSample of data after dummy transformation:\")\n",
    "print(df_train.head())\n",
    "print(\"New shape after creating dummy variables:\", df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in training data:\n",
      "Class 0 (Normal Traffic): 67343 samples (53.46%)\n",
      "Class 1 (DOS (Denial of Service)): 45927 samples (36.46%)\n",
      "Class 2 (Probe (Surveillance/Scanning)): 11656 samples (9.25%)\n",
      "Class 3 (R2L (Remote to Local)): 995 samples (0.79%)\n",
      "Class 4 (U2R (User to Root)): 52 samples (0.04%)\n",
      "\n",
      "Selected features: ['src_bytes', 'dst_bytes', 'logged_in', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate', 'diff_srv_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'service_http', 'flag_S0', 'flag_SF']\n",
      "Shape after SelectKBest: (125973, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create multiclass labels and apply SelectKBest\n",
    "def select_best_features(X, y, k=20):\n",
    "    \"\"\"\n",
    "    Select K best features using mutual information classification\n",
    "    \"\"\"\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    return X_new, selected_features, selector\n",
    "\n",
    "# Create multiclass labels\n",
    "df_train['multiclass_label'] = df_train['label'].map(attack_mapping)\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nClass distribution in training data:\")\n",
    "class_dist = df_train['multiclass_label'].value_counts().sort_index()\n",
    "for class_id, count in class_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(df_train)*100:.2f}%)\")\n",
    "\n",
    "# Apply SelectKBest\n",
    "X_train = df_train.drop(['label', 'multiclass_label'], axis=1)\n",
    "y_train = df_train['multiclass_label']\n",
    "X_train_selected, selected_features, selector = select_best_features(X_train, y_train)\n",
    "\n",
    "print(\"\\nSelected features:\", selected_features)\n",
    "print(\"Shape after SelectKBest:\", X_train_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Correlation Feature Selection...\n",
      "Initial shape: (125973, 124)\n",
      "Removing constant columns...\n",
      "Removed 1 constant columns\n",
      "Calculating correlation matrix...\n",
      "Getting target correlations...\n",
      "Selecting features...\n",
      "Selected 109 features\n",
      "\n",
      "Features selected by CFS: ['dst_host_srv_count', 'logged_in', 'flag_SF', 'service_http', 'service_private', 'dst_host_diff_srv_rate', 'count', 'dst_host_srv_serror_rate', 'service_eco_i', 'dst_host_same_src_port_rate', 'dst_host_srv_rerror_rate', 'protocol_type_icmp', 'diff_srv_rate', 'flag_RSTR', 'dst_host_srv_diff_host_rate', 'service_domain_u', 'dst_host_count', 'service_smtp', 'protocol_type_udp', 'duration', 'flag_SH', 'service_ecr_i', 'hot', 'flag_RSTO', 'service_other', 'flag_RSTOS0', 'protocol_type_tcp', 'service_urp_i', 'wrong_fragment', 'service_Z39_50', 'service_uucp', 'service_whois', 'service_imap4', 'service_courier', 'service_ftp', 'service_bgp', 'service_uucp_path', 'service_iso_tsap', 'service_ctf', 'service_gopher', 'service_vmnet', 'service_nnsp', 'service_discard', 'flag_S1', 'service_supdup', 'service_daytime', 'service_csnet_ns', 'service_link', 'service_name', 'service_http_443', 'service_systat', 'service_mtp', 'service_hostnames', 'service_echo', 'service_efs', 'service_exec', 'service_ftp_data', 'srv_count', 'service_domain', 'service_login', 'service_klogin', 'service_netbios_dgm', 'service_time', 'service_ldap', 'service_sunrpc', 'service_netstat', 'service_netbios_ssn', 'service_ssh', 'service_netbios_ns', 'service_nntp', 'service_kshell', 'service_IRC', 'service_ntp_u', 'num_access_files', 'service_sql_net', 'service_telnet', 'flag_OTH', 'service_finger', 'service_auth', 'service_rje', 'service_remote_job', 'num_failed_logins', 'flag_S2', 'su_attempted', 'service_pop_2', 'service_printer', 'service_shell', 'service_pop_3', 'num_file_creations', 'service_X11', 'src_bytes', 'service_pm_dump', 'srv_diff_host_rate', 'num_root', 'flag_S3', 'dst_bytes', 'service_http_8001', 'service_harvest', 'service_aol', 'root_shell', 'service_urh_i', 'service_red_i', 'service_http_2784', 'num_shells', 'service_tftp_u', 'urgent', 'land', 'is_host_login', 'service_tim_i']\n",
      "Shape after CFS: (125973, 110)\n",
      "\n",
      "Preprocessing complete!\n",
      "Processed training data saved to: /root/autodl-tmp/projects/SL_NSL/dataset/processed/multi/KDDTrain_processed.csv\n",
      "Preprocessing objects saved to: /root/autodl-tmp/projects/SL_NSL/dataset/processed/multi/preprocessing_objects.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Correlation Feature Selection (CFS)\n",
    "def correlation_feature_selection(df, target_col, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Select features based on correlation with target and between features\n",
    "    \"\"\"\n",
    "    # Make a copy of dataframe to avoid modifying original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Remove constant columns\n",
    "    print(\"Removing constant columns...\")\n",
    "    constant_columns = [col for col in df_copy.columns if df_copy[col].nunique() == 1]\n",
    "    df_copy = df_copy.drop(columns=constant_columns)\n",
    "    print(f\"Removed {len(constant_columns)} constant columns\")\n",
    "    \n",
    "    # Calculate correlations\n",
    "    print(\"Calculating correlation matrix...\")\n",
    "    corr_matrix = df_copy.corr(numeric_only=True).abs()\n",
    "    \n",
    "    print(\"Getting target correlations...\")\n",
    "    target_corr = corr_matrix[target_col].sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Selecting features...\")\n",
    "    selected_features = []\n",
    "    for feature in target_corr.index:\n",
    "        if feature == target_col:\n",
    "            continue\n",
    "            \n",
    "        include = True\n",
    "        for selected in selected_features:\n",
    "            if corr_matrix.loc[feature, selected] > threshold:\n",
    "                include = False\n",
    "                break\n",
    "                \n",
    "        if include:\n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "    \n",
    "    return df[selected_features + [target_col]], selected_features\n",
    "\n",
    "# Apply CFS\n",
    "print(\"\\nStarting Correlation Feature Selection...\")\n",
    "print(f\"Initial shape: {df_train.shape}\")\n",
    "df_train_cfs, cfs_features = correlation_feature_selection(df_train, 'multiclass_label')\n",
    "print(\"\\nFeatures selected by CFS:\", cfs_features)\n",
    "print(\"Shape after CFS:\", df_train_cfs.shape)\n",
    "\n",
    "# Cell 7: Save processed data and preprocessing objects\n",
    "# Save processed datasets\n",
    "train_processed_path = os.path.join(output_directory, 'KDDTrain_processed.csv')\n",
    "df_train_cfs.to_csv(train_processed_path, index=False)\n",
    "\n",
    "# Save preprocessing objects\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler,\n",
    "    'selector': selector,\n",
    "    'selected_features': selected_features,\n",
    "    'cfs_features': cfs_features,\n",
    "    'attack_mapping': attack_mapping,\n",
    "    'class_names': class_names\n",
    "}\n",
    "\n",
    "encoders_path = os.path.join(output_directory, 'preprocessing_objects.pkl')\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(f\"Processed training data saved to: {train_processed_path}\")\n",
    "print(f\"Preprocessing objects saved to: {encoders_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Processed training data saved to: /root/autodl-tmp/projects/SL_NSL/dataset/processed/multi/KDDTrain_processed.csv\n",
      "Preprocessing objects saved to: /root/autodl-tmp/projects/SL_NSL/dataset/processed/multi/preprocessing_objects.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save processed data and preprocessing objects\n",
    "# Save processed datasets\n",
    "train_processed_path = os.path.join(output_directory, 'KDDTrain_processed.csv')\n",
    "df_train_cfs.to_csv(train_processed_path, index=False)\n",
    "\n",
    "# Save preprocessing objects\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler,\n",
    "    'selector': selector,\n",
    "    'selected_features': selected_features,\n",
    "    'cfs_features': cfs_features,\n",
    "    'attack_mapping': attack_mapping,\n",
    "    'class_names': class_names\n",
    "}\n",
    "\n",
    "encoders_path = os.path.join(output_directory, 'preprocessing_objects.pkl')\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(f\"Processed training data saved to: {train_processed_path}\")\n",
    "print(f\"Preprocessing objects saved to: {encoders_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading testing dataset...\n",
      "Applying feature scaling...\n",
      "Applying dummy transformation...\n",
      "Creating multiclass labels...\n",
      "\n",
      "Class distribution in test data:\n",
      "Class 0.0 (Normal Traffic): 9711 samples (43.08%)\n",
      "Class 1.0 (DOS (Denial of Service)): 5741 samples (25.47%)\n",
      "Class 2.0 (Probe (Surveillance/Scanning)): 2421 samples (10.74%)\n",
      "Class 3.0 (R2L (Remote to Local)): 2199 samples (9.75%)\n",
      "Class 4.0 (U2R (User to Root)): 52 samples (0.23%)\n",
      "\n",
      "Aligning features with training set...\n",
      "Applying feature selection...\n",
      "\n",
      "Test set preprocessing complete!\n",
      "Processed test data saved to: /root/autodl-tmp/projects/SL_NSL/dataset/processed/multi/KDDTest_processed.csv\n",
      "\n",
      "Final dataset shapes:\n",
      "Training set: (125973, 110)\n",
      "Testing set: (22544, 110)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Process Test Dataset\n",
    "print(\"\\nLoading testing dataset...\")\n",
    "df_test = pd.read_csv(test_dataset_path)\n",
    "\n",
    "# 1. Apply feature scaling using training scaler\n",
    "print(\"Applying feature scaling...\")\n",
    "df_test, _ = feature_scaling(df_test, scaler=scaler)\n",
    "\n",
    "# 2. Apply get_dummies transformation\n",
    "print(\"Applying dummy transformation...\")\n",
    "df_test = get_dummies_transform(df_test)\n",
    "\n",
    "# 3. Create multiclass labels\n",
    "print(\"Creating multiclass labels...\")\n",
    "df_test['multiclass_label'] = df_test['label'].map(attack_mapping)\n",
    "\n",
    "# Print test set class distribution\n",
    "print(\"\\nClass distribution in test data:\")\n",
    "test_dist = df_test['multiclass_label'].value_counts().sort_index()\n",
    "for class_id, count in test_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(df_test)*100:.2f}%)\")\n",
    "\n",
    "# 4. Ensure all features from training set exist in test set\n",
    "print(\"\\nAligning features with training set...\")\n",
    "for col in df_train.columns:\n",
    "    if col not in df_test.columns and col != 'multiclass_label':\n",
    "        df_test[col] = 0\n",
    "\n",
    "# 5. Apply CFS with same features as training set\n",
    "print(\"Applying feature selection...\")\n",
    "df_test_processed = df_test[cfs_features + ['multiclass_label']]\n",
    "\n",
    "# Save processed test dataset\n",
    "test_processed_path = os.path.join(output_directory, 'KDDTest_processed.csv')\n",
    "df_test_processed.to_csv(test_processed_path, index=False)\n",
    "\n",
    "print(\"\\nTest set preprocessing complete!\")\n",
    "print(f\"Processed test data saved to: {test_processed_path}\")\n",
    "\n",
    "# Display final shapes\n",
    "print(\"\\nFinal dataset shapes:\")\n",
    "print(f\"Training set: {df_train_cfs.shape}\")\n",
    "print(f\"Testing set: {df_test_processed.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl-nsl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
